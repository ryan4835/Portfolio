# Mind the Spectral Gap
## The code from my Masters mini-project on Graph Convolutional Nets

The application of Convolutional Neural Networks to Audio-Visual learning tasks has been enormously successful, in many cases achieving super-human performance [1]. The success in part is due to the local translational invariance of data deﬁned over Euclidean domains. Increasingly we wish to learn functions over non-euclidean domains, such as graphs, leading to the development of Graph Convolutional Networks (GCNs). Despite the high performance of shallow GCNs, mysteriously, we experience diminishing returns with added depth. In this paper, we link the depth of a GCN to the well-studied spectral gap of a graph. In network science the spectral gap has been shown to strongly inﬂuence the mixing time of signals on a graph, and we argue it should be considered when determining the depth of GCN to use.
